{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "We will investigate Naive Bayes methods since we assume conditional independence between every pair of features (words) given the class.\n",
    "\n",
    "Going to do Bernoulli and Multinomial NB as is common for text classification. Not going to do Gaussian NB bc not continuous. Going to try complement naive Bayes (CNB) algorithm, an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets (https://scikit-learn.org/stable/modules/naive_bayes.html) on the not downsampled data.\n",
    "\n",
    "Going to use RandomizedSearch instead of GridSearchCV which \"can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import random\n",
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing labels\n",
    "with open('../data/train_labels.pckl', 'rb') as f:\n",
    "    train_labels = pickle.load(f)\n",
    "\n",
    "with open('../data/dev_labels.pckl', 'rb') as f:\n",
    "    dev_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing 2 of the 10 vectorizers to experiment with. Later import all. \n",
    "\n",
    "with open('../data/train_binary_downsampled_data.pckl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('../data/dev_binary_downsampled_data.pckl', 'rb') as f:\n",
    "    dev = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vectorizer):\n",
    "    '''\n",
    "    returns feature matrix for specified dataset and vectorizer\n",
    "    @param dataset: string specifying dataset, \"train\",\"dev\",etc\n",
    "    @param vectorizer: string specifying vectorizer \"binary\",\"count\",etc\n",
    "\n",
    "    '''\n",
    "    with open(f'../data/{dataset}_{vectorizer}_downsampled_data.pckl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial\n",
    "\"Empirical comparisons provide evidence that the multinomial model tends to outperform the multi-variate Bernoulli model if the vocabulary size is relatively large [13]. However, the performance of machine learning algorithms is highly dependent on the appropriate choice of features. In the case of naive Bayes classifiers and text classification, large differences in performance can be attributed to the choices of stop word removal, stemming, and token-length [14].\" (Citation: https://sebastianraschka.com/Articles/2014_naive_bayes_1.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  count  -----\n",
      "Train AUC: 0.9868\n",
      "Dev   AUC: 0.6616\n",
      "Train AP:  0.9784\n",
      "Dev   AP:  0.1584\n",
      "-----  tfidf  -----\n",
      "Train AUC: 0.9642\n",
      "Dev   AUC: 0.6906\n",
      "Train AP:  0.9564\n",
      "Dev   AP:  0.1667\n",
      "-----  binary  -----\n",
      "Train AUC: 0.9862\n",
      "Dev   AUC: 0.6622\n",
      "Train AP:  0.9776\n",
      "Dev   AP:  0.1597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizers = ['count', 'tfidf', 'binary'] # 'hashing', 'hashing_binary'\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'alpha': loguniform(1e-4, 1e0)}\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    print('----- ', vectorizer, ' -----')\n",
    "    train = get_data('train', vectorizer)\n",
    "    dev = get_data('dev', vectorizer)\n",
    "\n",
    "    nb_multi = MultinomialNB()  \n",
    "        \n",
    "    # run randomized search\n",
    "    random_search = RandomizedSearchCV(nb_multi, param_distributions=param_dist)\n",
    "    \n",
    "    random_search.fit(train, train_labels)\n",
    "    \n",
    "    nb_train = random_search.predict(train)\n",
    "    nb_dev = random_search.predict(dev)\n",
    "    \n",
    "    nb_train_auc = roc_auc_score(train_labels, nb_train)\n",
    "    nb_dev_auc = roc_auc_score(dev_labels, nb_dev)\n",
    "    nb_train_ap = average_precision_score(train_labels, nb_train)\n",
    "    nb_dev_ap = average_precision_score(dev_labels, nb_dev)\n",
    "    \n",
    "    print(f'Train AUC: {nb_train_auc:.4f}\\n'\n",
    "          f'Dev   AUC: {nb_dev_auc:.4f}\\n'\n",
    "          f'Train AP:  {nb_train_ap:.4f}\\n'\n",
    "          f'Dev   AP:  {nb_dev_ap:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli\n",
    "\n",
    "V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with Naive Bayes – Which Naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  count  -----\n",
      "Train AUC: 0.9680\n",
      "Dev   AUC: 0.6274\n",
      "Train AP:  0.9398\n",
      "Dev   AP:  0.1340\n",
      "-----  tfidf  -----\n",
      "Train AUC: 0.9745\n",
      "Dev   AUC: 0.6272\n",
      "Train AP:  0.9515\n",
      "Dev   AP:  0.1345\n",
      "-----  binary  -----\n",
      "Train AUC: 0.9699\n",
      "Dev   AUC: 0.6276\n",
      "Train AP:  0.9432\n",
      "Dev   AP:  0.1342\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "vectorizers = ['count', 'tfidf', 'binary'] # 'hashing', 'hashing_binary'\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'alpha': loguniform(1e-4, 1e0)}\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    print('----- ', vectorizer, ' -----')\n",
    "    train = get_data('train', vectorizer)\n",
    "    dev = get_data('dev', vectorizer)\n",
    "\n",
    "    nb_bern = BernoulliNB()  \n",
    "        \n",
    "    # run randomized search\n",
    "    random_search = RandomizedSearchCV(nb_bern, param_distributions=param_dist)\n",
    "    \n",
    "    random_search.fit(train, train_labels)\n",
    "    \n",
    "    nb_train = random_search.predict(train)\n",
    "    nb_dev = random_search.predict(dev)\n",
    "    \n",
    "    nb_train_auc = roc_auc_score(train_labels, nb_train)\n",
    "    nb_dev_auc = roc_auc_score(dev_labels, nb_dev)\n",
    "    nb_train_ap = average_precision_score(train_labels, nb_train)\n",
    "    nb_dev_ap = average_precision_score(dev_labels, nb_dev)\n",
    "    \n",
    "    print(f'Train AUC: {nb_train_auc:.4f}\\n'\n",
    "          f'Dev   AUC: {nb_dev_auc:.4f}\\n'\n",
    "          f'Train AP:  {nb_train_ap:.4f}\\n'\n",
    "          f'Dev   AP:  {nb_dev_ap:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non downsampled explorations\n",
    "\n",
    "### Complement NB\n",
    "The Complement Naive Bayes classifier was designed to correct the “severe assumptions” made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets. [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB)\n",
    "\n",
    "This was done with non-downsampled data that I generated by: not running downsample.ipynb then editing file names and re-running vectorize-count.ipynb and concat-features.ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_not_ds(dataset, vectorizer):\n",
    "    '''\n",
    "    get data that is not downsampled\n",
    "    @param dataset: string specifying dataset, \"train\",\"dev\",etc\n",
    "    @param vectorizer: string specifying vectorizer \"binary\",\"count\",etc\n",
    "\n",
    "    '''\n",
    "    with open(f'../data/{dataset}_{vectorizer}_NOT_downsampled_data.pckl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing labels NOT DOWNSAMPLED\n",
    "\n",
    "with open('../data/train_labels_nods.pckl', 'rb') as f:\n",
    "    train_labels_not_ds = pickle.load(f)\n",
    "\n",
    "with open('../data/dev_labels_nods.pckl', 'rb') as f:\n",
    "    dev_labels_not_ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  count  -----\n",
      "Train AUC: 0.8738\n",
      "Dev   AUC: 0.5043\n",
      "Train AP:  0.7713\n",
      "Dev   AP:  0.1066\n",
      "-----  tfidf  -----\n",
      "Train AUC: 0.5013\n",
      "Dev   AUC: 0.5000\n",
      "Train AP:  0.1052\n",
      "Dev   AP:  0.1016\n",
      "-----  binary  -----\n",
      "Train AUC: 0.6449\n",
      "Dev   AUC: 0.5022\n",
      "Train AP:  0.3624\n",
      "Dev   AP:  0.1052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "vectorizers = ['count', 'tfidf', 'binary'] # 'hashing', 'hashing_binary'\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'alpha': loguniform(1e-4, 1e0)}\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    print('----- ', vectorizer, ' -----')\n",
    "    train = get_data_not_ds('train', vectorizer)\n",
    "    dev = get_data_not_ds('dev', vectorizer)\n",
    "\n",
    "    nb_comp = ComplementNB()  \n",
    "        \n",
    "    # run randomized search\n",
    "    random_search = RandomizedSearchCV(nb_comp, param_distributions=param_dist)\n",
    "    \n",
    "    random_search.fit(train, train_labels_not_ds)\n",
    "    \n",
    "    nb_train = random_search.predict(train)\n",
    "    nb_dev = random_search.predict(dev)\n",
    "    \n",
    "    nb_train_auc = roc_auc_score(train_labels_not_ds, nb_train)\n",
    "    nb_dev_auc = roc_auc_score(dev_labels_not_ds, nb_dev)\n",
    "    nb_train_ap = average_precision_score(train_labels_not_ds, nb_train)\n",
    "    nb_dev_ap = average_precision_score(dev_labels_not_ds, nb_dev)\n",
    "    \n",
    "    print(f'Train AUC: {nb_train_auc:.4f}\\n'\n",
    "          f'Dev   AUC: {nb_dev_auc:.4f}\\n'\n",
    "          f'Train AP:  {nb_train_ap:.4f}\\n'\n",
    "          f'Dev   AP:  {nb_dev_ap:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying other 2 NB models with not downsampled data\n",
    "Curious if downsampling worsens performance with other models too.\n",
    "#### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  count  -----\n",
      "Train AUC: 0.5353\n",
      "Dev   AUC: 0.5001\n",
      "Train AP:  0.1393\n",
      "Dev   AP:  0.1017\n",
      "-----  tfidf  -----\n",
      "Train AUC: 0.7144\n",
      "Dev   AUC: 0.5021\n",
      "Train AP:  0.4700\n",
      "Dev   AP:  0.1038\n",
      "-----  binary  -----\n",
      "Train AUC: 0.5286\n",
      "Dev   AUC: 0.5001\n",
      "Train AP:  0.1309\n",
      "Dev   AP:  0.1017\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli with raw (non-downsampled) data\n",
    "vectorizers = ['count', 'tfidf', 'binary'] # 'hashing', 'hashing_binary'\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'alpha': loguniform(1e-4, 1e0)}\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    print('----- ', vectorizer, ' -----')\n",
    "    train = get_data_not_ds('train', vectorizer)\n",
    "    dev = get_data_not_ds('dev', vectorizer)\n",
    "\n",
    "    nb_bern = BernoulliNB()  \n",
    "        \n",
    "    # run randomized search\n",
    "    random_search = RandomizedSearchCV(nb_bern, param_distributions=param_dist)\n",
    "    \n",
    "    random_search.fit(train, train_labels_not_ds)\n",
    "    \n",
    "    nb_train = random_search.predict(train)\n",
    "    nb_dev = random_search.predict(dev)\n",
    "    \n",
    "    nb_train_auc = roc_auc_score(train_labels_not_ds, nb_train)\n",
    "    nb_dev_auc = roc_auc_score(dev_labels_not_ds, nb_dev)\n",
    "    nb_train_ap = average_precision_score(train_labels_not_ds, nb_train)\n",
    "    nb_dev_ap = average_precision_score(dev_labels_not_ds, nb_dev)\n",
    "    \n",
    "    print(f'Train AUC: {nb_train_auc:.4f}\\n'\n",
    "          f'Dev   AUC: {nb_dev_auc:.4f}\\n'\n",
    "          f'Train AP:  {nb_train_ap:.4f}\\n'\n",
    "          f'Dev   AP:  {nb_dev_ap:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  count  -----\n",
      "Train AUC: 0.9122\n",
      "Dev   AUC: 0.5049\n",
      "Train AP:  0.8391\n",
      "Dev   AP:  0.1070\n",
      "-----  tfidf  -----\n",
      "Train AUC: 0.5209\n",
      "Dev   AUC: 0.5001\n",
      "Train AP:  0.1404\n",
      "Dev   AP:  0.1018\n",
      "-----  binary  -----\n",
      "Train AUC: 0.6442\n",
      "Dev   AUC: 0.5019\n",
      "Train AP:  0.3612\n",
      "Dev   AP:  0.1048\n"
     ]
    }
   ],
   "source": [
    "# Multinomial with raw (non-downsampled) data\n",
    "vectorizers = ['count', 'tfidf', 'binary'] # 'hashing', 'hashing_binary'\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'alpha': loguniform(1e-4, 1e0)}\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    print('----- ', vectorizer, ' -----')\n",
    "    train = get_data_not_ds('train', vectorizer)\n",
    "    dev = get_data_not_ds('dev', vectorizer)\n",
    "\n",
    "    nb_multi = MultinomialNB()  \n",
    "        \n",
    "    # run randomized search\n",
    "    random_search = RandomizedSearchCV(nb_multi, param_distributions=param_dist)\n",
    "    \n",
    "    random_search.fit(train, train_labels_not_ds)\n",
    "    \n",
    "    nb_train = random_search.predict(train)\n",
    "    nb_dev = random_search.predict(dev)\n",
    "    \n",
    "    nb_train_auc = roc_auc_score(train_labels_not_ds, nb_train)\n",
    "    nb_dev_auc = roc_auc_score(dev_labels_not_ds, nb_dev)\n",
    "    nb_train_ap = average_precision_score(train_labels_not_ds, nb_train)\n",
    "    nb_dev_ap = average_precision_score(dev_labels_not_ds, nb_dev)\n",
    "    \n",
    "    print(f'Train AUC: {nb_train_auc:.4f}\\n'\n",
    "          f'Dev   AUC: {nb_dev_auc:.4f}\\n'\n",
    "          f'Train AP:  {nb_train_ap:.4f}\\n'\n",
    "          f'Dev   AP:  {nb_dev_ap:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
